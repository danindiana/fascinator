import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the environment (simplified)
class Environment:
    def __init__(self):
        self.state = np.random.rand(4)
    
    def step(self, action):
        # Simulate environment dynamics
        self.state += action
        reward = np.random.rand()
        done = False
        return self.state, reward, done

# Define the agent
class Agent:
    def __init__(self):
        self.model = self.build_model()
    
    def build_model(self):
        model = Sequential([
            Dense(32, input_dim=4, activation='relu'),
            Dense(32, activation='relu'),
            Dense(2, activation='linear')
        ])
        model.compile(loss='mse', optimizer='adam')
        return model
    
    def get_action(self, state):
        state = state.reshape(1, -1)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])
    
    def train(self, state, action, reward, next_state, done):
        target = reward + (1 - done) * np.max(self.model.predict(next_state.reshape(1, -1)))
        target_f = self.model.predict(state.reshape(1, -1))
        target_f[0][action] = target
        self.model.fit(state.reshape(1, -1), target_f, epochs=1, verbose=0)

# Intrinsic Curiosity Module (simplified)
class ICM:
    def __init__(self):
        self.forward_model = self.build_forward_model()
        self.inverse_model = self.build_inverse_model()
    
    def build_forward_model(self):
        model = Sequential([
            Dense(32, input_dim=6, activation='relu'),
            Dense(4, activation='linear')
        ])
        model.compile(loss='mse', optimizer='adam')
        return model
    
    def build_inverse_model(self):
        model = Sequential([
            Dense(32, input_dim=8, activation='relu'),
            Dense(2, activation='softmax')
        ])
        model.compile(loss='categorical_crossentropy', optimizer='adam')
        return model
    
    def compute_intrinsic_reward(self, state, action, next_state):
        state_action = np.concatenate([state, action], axis=0)
        next_state_pred = self.forward_model.predict(state_action.reshape(1, -1))
        intrinsic_reward = np.mean((next_state - next_state_pred) ** 2)
        return intrinsic_reward

# Training loop
env = Environment()
agent = Agent()
icm = ICM()

for episode in range(1000):
    state = env.state
    done = False
    while not done:
        action = agent.get_action(state)
        next_state, reward, done = env.step(action)
        intrinsic_reward = icm.compute_intrinsic_reward(state, action, next_state)
        total_reward = reward + intrinsic_reward
        agent.train(state, action, total_reward, next_state, done)
        state = next_state
